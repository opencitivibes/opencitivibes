#!/bin/bash
# Initial reconnaissance script

source /app/scripts/helpers/colors.sh
print_banner

TARGET_API="${TARGET_API:-http://backend:8000/api}"
SCAN_DIR="${SCAN_DIR:-/app/results/$(date +%Y%m%d_%H%M%S)}"
mkdir -p "${SCAN_DIR}/recon"

log_info "Starting reconnaissance against ${TARGET_API}"
log_info "Results will be saved to ${SCAN_DIR}/recon"

# 1. API Endpoint Discovery
log_info "Discovering API endpoints..."

# Check OpenAPI/Swagger docs
curl -s "${TARGET_API}/../docs" > "${SCAN_DIR}/recon/swagger.html" 2>/dev/null
curl -s "${TARGET_API}/../redoc" > "${SCAN_DIR}/recon/redoc.html" 2>/dev/null
curl -s "${TARGET_API}/../openapi.json" > "${SCAN_DIR}/recon/openapi.json" 2>/dev/null

if [ -s "${SCAN_DIR}/recon/openapi.json" ]; then
    log_success "Found OpenAPI specification"
    # Extract endpoints from OpenAPI
    jq -r '.paths | keys[]' "${SCAN_DIR}/recon/openapi.json" > "${SCAN_DIR}/recon/endpoints.txt"
    log_info "Extracted $(wc -l < ${SCAN_DIR}/recon/endpoints.txt) endpoints"
else
    log_warning "No OpenAPI specification found"
fi

# 2. Technology fingerprinting
log_info "Fingerprinting technologies..."
whatweb -a 3 "${TARGET_API}/../" --log-json="${SCAN_DIR}/recon/whatweb.json" 2>/dev/null || log_warning "whatweb scan failed"

# 3. HTTP Headers analysis
log_info "Analyzing HTTP headers..."
curl -sI "${TARGET_API}/health" > "${SCAN_DIR}/recon/headers.txt"

# Check security headers
log_info "Checking security headers..."
{
    echo "=== Security Headers Analysis ==="
    echo ""

    headers=$(cat "${SCAN_DIR}/recon/headers.txt")

    # Check for important security headers
    for header in "Strict-Transport-Security" "X-Content-Type-Options" "X-Frame-Options" "X-XSS-Protection" "Content-Security-Policy" "Referrer-Policy"; do
        if echo "$headers" | grep -qi "$header"; then
            echo "[+] $header: Present"
        else
            echo "[-] $header: MISSING"
        fi
    done
} > "${SCAN_DIR}/recon/security-headers-analysis.txt"

cat "${SCAN_DIR}/recon/security-headers-analysis.txt"

# 4. WAF Detection
log_info "Detecting WAF..."
wafw00f "${TARGET_API}/../" -o "${SCAN_DIR}/recon/waf.json" -f json 2>/dev/null

# 5. Port scanning (if nmap accessible)
log_info "Running port scan..."
nmap -sV -sC -oA "${SCAN_DIR}/recon/nmap" backend 2>/dev/null || log_warning "Nmap scan failed"

# 6. Directory enumeration (quick)
log_info "Quick directory enumeration..."
gobuster dir -u "${TARGET_API}/../" \
    -w /usr/share/seclists/Discovery/Web-Content/api/api-endpoints.txt \
    -o "${SCAN_DIR}/recon/gobuster.txt" \
    -q -t 20 --timeout 5s 2>/dev/null || log_warning "Gobuster scan incomplete"

# 7. Playwright SPA Crawling (if available)
if command -v playwright &> /dev/null || python3 -c "import playwright" 2>/dev/null; then
    log_info "Running Playwright SPA crawler..."

    # Determine credentials for authenticated crawl
    CRAWL_CREDS=""
    if [ -n "${ADMIN_EMAIL}" ] && [ -n "${ADMIN_PASSWORD}" ]; then
        CRAWL_CREDS="--login ${ADMIN_EMAIL}:${ADMIN_PASSWORD}"
    fi

    # Run crawler with appropriate depth based on scan mode
    MAX_DEPTH="${PLAYWRIGHT_MAX_DEPTH:-3}"
    if [ "${SCAN_MODE}" = "quick" ]; then
        MAX_DEPTH=1
    fi

    if [ -f "/app/scripts/run-playwright-crawl.py" ]; then
        python3 /app/scripts/run-playwright-crawl.py \
            --start-url "${TARGET_FRONTEND:-http://frontend:3000}" \
            --output-file "${SCAN_DIR}/recon/playwright-urls.txt" \
            --max-depth ${MAX_DEPTH} \
            --headless true \
            ${CRAWL_CREDS} 2>&1 | tee "${SCAN_DIR}/recon/playwright.log"

        if [ -f "${SCAN_DIR}/recon/playwright-urls.txt" ]; then
            CRAWLED_COUNT=$(wc -l < "${SCAN_DIR}/recon/playwright-urls.txt")
            log_success "Playwright discovered ${CRAWLED_COUNT} URLs"

            # Deduplicate with API endpoints
            if [ -f "${SCAN_DIR}/recon/endpoints.txt" ]; then
                log_info "Merging with API endpoints..."
                cat "${SCAN_DIR}/recon/endpoints.txt" "${SCAN_DIR}/recon/playwright-urls.txt" | \
                    sort -u > "${SCAN_DIR}/recon/all-urls.txt"
                log_info "Total unique URLs: $(wc -l < ${SCAN_DIR}/recon/all-urls.txt)"
            fi
        else
            log_warning "Playwright crawl produced no output"
        fi
    else
        log_warning "Playwright crawl script not found"
    fi
else
    log_warning "Playwright not available, skipping SPA crawl"
fi

# 8. Normalize and dedupe all discovered URLs
log_info "Normalizing discovered URLs..."
if [ -f "/app/scripts/helpers/dedupe-urls.sh" ] && [ -f "${SCAN_DIR}/recon/all-urls.txt" ]; then
    /app/scripts/helpers/dedupe-urls.sh \
        "${SCAN_DIR}/recon/all-urls.txt" \
        "${SCAN_DIR}/recon/normalized-urls.txt" \
        "${MAX_URLS:-500}"
fi

# Summary
log_success "Reconnaissance complete!"
echo ""
log_info "Results saved to: ${SCAN_DIR}/recon/"
ls -la "${SCAN_DIR}/recon/"
