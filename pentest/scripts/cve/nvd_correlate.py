#!/usr/bin/env python3
"""
NVD Database Correlation for CVE findings.

Enriches scan results with authoritative NVD data including:
- CVSS 3.1 scores and vectors
- CWE classifications
- Vulnerability descriptions
- Reference links

Usage:
    SCAN_DIR=/app/results/cve python nvd_correlate.py
"""

import json
import os
import re
import requests
import time
from pathlib import Path
from typing import Dict, List, Optional, Any
from datetime import datetime

NVD_API_URL = "https://services.nvd.nist.gov/rest/json/cves/2.0"
SCAN_DIR = os.environ.get("SCAN_DIR", "/app/results/cve")
CACHE_DIR = Path(SCAN_DIR) / "nvd_cache"


class NVDCorrelator:
    def __init__(self, api_key: Optional[str] = None):
        self.api_key = api_key or os.environ.get("NVD_API_KEY")
        self.cache_dir = CACHE_DIR
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        # Rate limit: 5 requests per 30 seconds without key, 50 with key
        self.rate_limit_delay = 0.6 if not self.api_key else 0.1
        self.cve_data = {}

    def lookup_cve(self, cve_id: str) -> Optional[Dict]:
        """Lookup CVE details from NVD with caching."""
        # Check cache first
        cache_file = self.cache_dir / f"{cve_id}.json"
        if cache_file.exists():
            try:
                with open(cache_file, "r") as f:
                    return json.load(f)
            except json.JSONDecodeError:
                pass  # Cache corrupted, re-fetch

        # Rate limit
        time.sleep(self.rate_limit_delay)

        try:
            headers = {"Accept": "application/json"}
            if self.api_key:
                headers["apiKey"] = self.api_key

            response = requests.get(
                NVD_API_URL,
                params={"cveId": cve_id},
                headers=headers,
                timeout=30
            )

            if response.status_code == 200:
                data = response.json()
                if data.get("vulnerabilities"):
                    cve_data = data["vulnerabilities"][0]["cve"]

                    result = self._parse_cve_data(cve_id, cve_data)

                    # Cache result
                    with open(cache_file, "w") as f:
                        json.dump(result, f, indent=2)

                    return result

            elif response.status_code == 404:
                print(f"    [!] CVE not found in NVD: {cve_id}")
            elif response.status_code == 403:
                print(f"    [!] Rate limited by NVD API, waiting...")
                time.sleep(30)
                return self.lookup_cve(cve_id)  # Retry
            else:
                print(f"    [!] NVD API error {response.status_code} for {cve_id}")

        except requests.exceptions.Timeout:
            print(f"    [!] Timeout looking up {cve_id}")
        except Exception as e:
            print(f"    [!] Error looking up {cve_id}: {e}")

        return None

    def _parse_cve_data(self, cve_id: str, cve_data: Dict) -> Dict:
        """Parse NVD CVE data into standardized format."""
        # Extract CVSS scores
        cvss = self._extract_cvss(cve_data)

        # Extract description
        descriptions = cve_data.get("descriptions", [])
        description = ""
        for desc in descriptions:
            if desc.get("lang") == "en":
                description = desc.get("value", "")
                break

        # Extract CWE
        cwe_ids = []
        for weakness in cve_data.get("weaknesses", []):
            for desc in weakness.get("description", []):
                if desc.get("value", "").startswith("CWE-"):
                    cwe_ids.append(desc["value"])

        # Extract references
        references = [
            {"url": ref.get("url"), "source": ref.get("source")}
            for ref in cve_data.get("references", [])
        ]

        return {
            "id": cve_id,
            "description": description[:500] if description else "",  # Truncate
            "published": cve_data.get("published", ""),
            "lastModified": cve_data.get("lastModified", ""),
            "cvss": cvss,
            "cwe_ids": cwe_ids,
            "references": references[:10],  # Limit references
            "nvd_url": f"https://nvd.nist.gov/vuln/detail/{cve_id}"
        }

    def _extract_cvss(self, cve_data: Dict) -> Dict:
        """Extract CVSS information, preferring v3.1."""
        metrics = cve_data.get("metrics", {})

        for version in ["cvssMetricV31", "cvssMetricV30", "cvssMetricV2"]:
            if version in metrics and metrics[version]:
                cvss_data = metrics[version][0]
                cvss = cvss_data.get("cvssData", {})
                return {
                    "version": cvss.get("version", ""),
                    "score": cvss.get("baseScore", 0),
                    "severity": cvss.get("baseSeverity", ""),
                    "vector": cvss.get("vectorString", ""),
                    "exploitabilityScore": cvss_data.get("exploitabilityScore", 0),
                    "impactScore": cvss_data.get("impactScore", 0)
                }

        return {"score": 0, "severity": "UNKNOWN"}

    def parse_trivy_results(self, json_file: Path) -> List[str]:
        """Extract CVE IDs from Trivy JSON results."""
        cves = []
        try:
            with open(json_file, "r") as f:
                data = json.load(f)

            results = data.get("Results", [])
            for result in results:
                for vuln in result.get("Vulnerabilities", []):
                    cve_id = vuln.get("VulnerabilityID", "")
                    if cve_id.startswith("CVE-"):
                        cves.append(cve_id)
        except Exception as e:
            print(f"    [!] Error parsing Trivy results {json_file}: {e}")

        return list(set(cves))

    def parse_grype_results(self, json_file: Path) -> List[str]:
        """Extract CVE IDs from Grype JSON results."""
        cves = []
        try:
            with open(json_file, "r") as f:
                data = json.load(f)

            matches = data.get("matches", [])
            for match in matches:
                cve_id = match.get("vulnerability", {}).get("id", "")
                if cve_id.startswith("CVE-"):
                    cves.append(cve_id)
        except Exception as e:
            print(f"    [!] Error parsing Grype results {json_file}: {e}")

        return list(set(cves))

    def parse_pip_audit_results(self, json_file: Path) -> List[str]:
        """Extract CVE IDs from pip-audit JSON results."""
        cves = []
        try:
            with open(json_file, "r") as f:
                data = json.load(f)

            # pip-audit can return list or dict depending on version
            vulns = data if isinstance(data, list) else data.get("vulnerabilities", [])
            for vuln in vulns:
                for alias in vuln.get("aliases", []):
                    if alias.startswith("CVE-"):
                        cves.append(alias)
        except Exception as e:
            print(f"    [!] Error parsing pip-audit results {json_file}: {e}")

        return list(set(cves))

    def parse_npm_audit_results(self, json_file: Path) -> List[str]:
        """Extract CVE IDs from npm/pnpm audit JSON results."""
        cves = []
        try:
            with open(json_file, "r") as f:
                data = json.load(f)

            # npm audit format
            advisories = data.get("advisories", {})
            for advisory_id, advisory in advisories.items():
                for cve in advisory.get("cves", []):
                    if cve.startswith("CVE-"):
                        cves.append(cve)

            # pnpm audit format
            for vuln in data.get("vulnerabilities", {}).values():
                via = vuln.get("via", [])
                for item in via:
                    if isinstance(item, dict):
                        url = item.get("url", "")
                        # Extract CVE from URL
                        match = re.search(r"CVE-\d{4}-\d{4,7}", url)
                        if match:
                            cves.append(match.group())

        except Exception as e:
            print(f"    [!] Error parsing npm audit results {json_file}: {e}")

        return list(set(cves))

    def correlate_all(self) -> Dict[str, Any]:
        """Correlate all CVEs from scan results with NVD."""
        scan_dir = Path(SCAN_DIR)
        all_cves = set()

        print("[*] Collecting CVEs from scan results...")

        # Collect from Trivy results
        for trivy_file in scan_dir.glob("**/trivy-*.json"):
            cves = self.parse_trivy_results(trivy_file)
            all_cves.update(cves)
            print(f"    {trivy_file.name}: {len(cves)} CVEs")

        # Collect from Grype results
        for grype_file in scan_dir.glob("**/grype-*.json"):
            cves = self.parse_grype_results(grype_file)
            all_cves.update(cves)
            print(f"    {grype_file.name}: {len(cves)} CVEs")

        # Collect from pip-audit
        for pip_file in scan_dir.glob("**/pip-audit.json"):
            cves = self.parse_pip_audit_results(pip_file)
            all_cves.update(cves)
            print(f"    {pip_file.name}: {len(cves)} CVEs")

        # Collect from npm/pnpm audit
        for npm_file in scan_dir.glob("**/*-audit.json"):
            if "pip" not in npm_file.name:
                cves = self.parse_npm_audit_results(npm_file)
                all_cves.update(cves)
                print(f"    {npm_file.name}: {len(cves)} CVEs")

        print(f"\n[*] Total unique CVEs: {len(all_cves)}")

        if not all_cves:
            print("[*] No CVEs found to correlate")
            return {
                "correlation_date": datetime.now().isoformat(),
                "total_cves": 0,
                "by_severity": {"CRITICAL": 0, "HIGH": 0, "MEDIUM": 0, "LOW": 0},
                "cves": []
            }

        print("[*] Looking up CVE details from NVD...")

        enriched_cves = []
        for i, cve_id in enumerate(sorted(all_cves)):
            print(f"    [{i+1}/{len(all_cves)}] {cve_id}")
            cve_data = self.lookup_cve(cve_id)
            if cve_data:
                enriched_cves.append(cve_data)

        # Sort by CVSS score
        enriched_cves.sort(
            key=lambda x: x.get("cvss", {}).get("score", 0),
            reverse=True
        )

        return {
            "correlation_date": datetime.now().isoformat(),
            "total_cves": len(enriched_cves),
            "by_severity": {
                "CRITICAL": len([c for c in enriched_cves if c.get("cvss", {}).get("score", 0) >= 9.0]),
                "HIGH": len([c for c in enriched_cves if 7.0 <= c.get("cvss", {}).get("score", 0) < 9.0]),
                "MEDIUM": len([c for c in enriched_cves if 4.0 <= c.get("cvss", {}).get("score", 0) < 7.0]),
                "LOW": len([c for c in enriched_cves if 0 < c.get("cvss", {}).get("score", 0) < 4.0])
            },
            "cves": enriched_cves
        }


def main():
    print("=" * 50)
    print("NVD Database Correlation")
    print("=" * 50)

    correlator = NVDCorrelator()
    results = correlator.correlate_all()

    output_file = Path(SCAN_DIR) / "nvd_correlation.json"
    with open(output_file, "w") as f:
        json.dump(results, f, indent=2)

    print(f"\n[+] NVD correlation complete!")
    print(f"    Total CVEs: {results['total_cves']}")
    print(f"    Critical: {results['by_severity']['CRITICAL']}")
    print(f"    High: {results['by_severity']['HIGH']}")
    print(f"    Medium: {results['by_severity']['MEDIUM']}")
    print(f"    Low: {results['by_severity']['LOW']}")
    print(f"    Output: {output_file}")


if __name__ == "__main__":
    main()
