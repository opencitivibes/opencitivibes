#!/usr/bin/env python3
"""
Aggregates findings from all scanning and testing phases into a unified format.

Usage:
    python aggregate_findings.py --scan-dir /app/results/20260102_105341

IMPORTANT: Always specify --scan-dir to avoid mixing results from different scans.
"""

import argparse
import json
import os
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Optional
from dataclasses import dataclass, asdict, field


@dataclass
class Finding:
    id: str
    title: str
    severity: str  # CRITICAL, HIGH, MEDIUM, LOW, INFO
    category: str  # SQLi, XSS, Auth, Config, etc.
    description: str
    affected_component: str
    reproduction_steps: List[str]
    evidence: List[str]
    remediation: str
    cwe_id: Optional[str] = None
    cvss_score: Optional[float] = None
    cvss_vector: Optional[str] = None
    references: List[str] = field(default_factory=list)
    source_tool: str = ""
    raw_finding: Optional[Dict] = None


class FindingsAggregator:
    def __init__(self, scan_dir: str):
        self.scan_dir = Path(scan_dir)
        self.findings: List[Finding] = []
        self.finding_counter = 0

    def generate_id(self) -> str:
        self.finding_counter += 1
        return f"OCV-2024-{self.finding_counter:04d}"

    def parse_nuclei_results(self, json_file: Path) -> List[Finding]:
        """Parse Nuclei JSON output."""
        findings = []
        try:
            with open(json_file, "r") as f:
                for line in f:
                    try:
                        result = json.loads(line)
                        finding = Finding(
                            id=self.generate_id(),
                            title=result.get("info", {}).get("name", "Unknown"),
                            severity=result.get("info", {}).get("severity", "INFO").upper(),
                            category="Vulnerability",
                            description=result.get("info", {}).get("description", ""),
                            affected_component=result.get("matched-at", ""),
                            reproduction_steps=[f"Access: {result.get('matched-at', '')}"],
                            evidence=[result.get("matcher-name", "")],
                            remediation=result.get("info", {}).get("remediation", "See references"),
                            cwe_id=result.get("info", {}).get("classification", {}).get("cwe-id", [None])[0] if result.get("info", {}).get("classification") else None,
                            references=result.get("info", {}).get("reference", []),
                            source_tool="Nuclei",
                            raw_finding=result
                        )
                        findings.append(finding)
                    except json.JSONDecodeError:
                        continue
        except Exception as e:
            print(f"Error parsing Nuclei results: {e}")
        return findings

    def parse_bandit_results(self, json_file: Path) -> List[Finding]:
        """Parse Bandit JSON output."""
        findings = []
        try:
            with open(json_file, "r") as f:
                data = json.load(f)

            for result in data.get("results", []):
                severity_map = {"HIGH": "HIGH", "MEDIUM": "MEDIUM", "LOW": "LOW"}
                finding = Finding(
                    id=self.generate_id(),
                    title=result.get("issue_text", "Security Issue"),
                    severity=severity_map.get(result.get("issue_severity", "LOW"), "LOW"),
                    category="Code Security",
                    description=result.get("issue_text", ""),
                    affected_component=f"{result.get('filename', '')}:{result.get('line_number', '')}",
                    reproduction_steps=[f"Review code at line {result.get('line_number', '')}"],
                    evidence=[result.get("code", "")],
                    remediation=f"See CWE-{result.get('issue_cwe', {}).get('id', 'N/A')} for remediation",
                    cwe_id=f"CWE-{result.get('issue_cwe', {}).get('id', '')}" if result.get('issue_cwe') else None,
                    references=[result.get("more_info", "")],
                    source_tool="Bandit",
                    raw_finding=result
                )
                findings.append(finding)
        except Exception as e:
            print(f"Error parsing Bandit results: {e}")
        return findings

    def parse_semgrep_results(self, json_file: Path) -> List[Finding]:
        """Parse Semgrep JSON output."""
        findings = []
        try:
            with open(json_file, "r") as f:
                data = json.load(f)

            for result in data.get("results", []):
                severity_map = {"ERROR": "HIGH", "WARNING": "MEDIUM", "INFO": "LOW"}
                finding = Finding(
                    id=self.generate_id(),
                    title=result.get("check_id", "Security Issue"),
                    severity=severity_map.get(result.get("extra", {}).get("severity", "INFO"), "INFO"),
                    category="Code Security",
                    description=result.get("extra", {}).get("message", ""),
                    affected_component=f"{result.get('path', '')}:{result.get('start', {}).get('line', '')}",
                    reproduction_steps=[f"Review code at {result.get('path', '')}"],
                    evidence=[result.get("extra", {}).get("lines", "")],
                    remediation="Review and fix the identified pattern",
                    source_tool="Semgrep",
                    raw_finding=result
                )
                findings.append(finding)
        except Exception as e:
            print(f"Error parsing Semgrep results: {e}")
        return findings

    def parse_zap_results(self, json_file: Path) -> List[Finding]:
        """Parse OWASP ZAP JSON output."""
        findings = []
        try:
            with open(json_file, "r") as f:
                data = json.load(f)

            for site in data.get("site", []):
                for alert in site.get("alerts", []):
                    risk_map = {"3": "CRITICAL", "2": "HIGH", "1": "MEDIUM", "0": "LOW"}
                    finding = Finding(
                        id=self.generate_id(),
                        title=alert.get("name", "Unknown"),
                        severity=risk_map.get(str(alert.get("riskcode", "0")), "INFO"),
                        category="Web Vulnerability",
                        description=alert.get("desc", ""),
                        affected_component=alert.get("instances", [{}])[0].get("uri", "") if alert.get("instances") else "",
                        reproduction_steps=[alert.get("instances", [{}])[0].get("method", "") + " " + alert.get("instances", [{}])[0].get("uri", "")] if alert.get("instances") else [],
                        evidence=[alert.get("instances", [{}])[0].get("evidence", "")] if alert.get("instances") else [],
                        remediation=alert.get("solution", ""),
                        cwe_id=f"CWE-{alert.get('cweid', '')}" if alert.get('cweid') else None,
                        references=alert.get("reference", "").split("\n") if alert.get("reference") else [],
                        source_tool="OWASP ZAP",
                        raw_finding=alert
                    )
                    findings.append(finding)
        except Exception as e:
            print(f"Error parsing ZAP results: {e}")
        return findings

    def parse_custom_results(self, json_file: Path) -> List[Finding]:
        """Parse custom test results."""
        findings = []
        try:
            with open(json_file, "r") as f:
                data = json.load(f)

            for result in data if isinstance(data, list) else data.get("results", data.get("findings", [])):
                if isinstance(result, dict) and result.get("level") in ["CRITICAL", "HIGH", "MEDIUM"]:
                    finding = Finding(
                        id=self.generate_id(),
                        title=result.get("message", "Security Issue"),
                        severity=result.get("level", "INFO"),
                        category="Custom Test",
                        description=result.get("message", ""),
                        affected_component=result.get("file", "Unknown"),
                        reproduction_steps=["See test output for details"],
                        evidence=[],
                        remediation="Review and address the identified issue",
                        source_tool="Custom Test",
                        raw_finding=result
                    )
                    findings.append(finding)
        except Exception as e:
            print(f"Error parsing custom results: {e}")
        return findings

    def parse_schemathesis_results(self, json_file: Path) -> List[Finding]:
        """Parse Schemathesis API fuzzing results."""
        findings = []
        try:
            with open(json_file, "r") as f:
                data = json.load(f)

            # Schemathesis outputs failures in various formats
            failures = data.get("failures", [])
            if not failures and isinstance(data, dict):
                # Try to extract from different structures
                for key in ["results", "checks", "tests"]:
                    if key in data:
                        failures = [
                            r for r in data[key]
                            if r.get("status") == "failure" or r.get("passed") is False
                        ]
                        break

            for failure in failures:
                # Determine severity based on failure type
                failure_type = failure.get("check", failure.get("type", "unknown"))
                severity = "MEDIUM"

                if "status_code" in failure_type.lower() or "500" in str(failure):
                    severity = "HIGH"
                if "security" in failure_type.lower() or "injection" in failure_type.lower():
                    severity = "CRITICAL"
                if "schema" in failure_type.lower():
                    severity = "LOW"

                endpoint = failure.get("path", failure.get("endpoint", "Unknown"))
                method = failure.get("method", "").upper()

                finding = Finding(
                    id=self.generate_id(),
                    title=f"API Fuzzing: {failure_type}",
                    severity=severity,
                    category="API Security",
                    description=failure.get("message", failure.get("description", f"Schemathesis found issue: {failure_type}")),
                    affected_component=f"{method} {endpoint}" if method else endpoint,
                    reproduction_steps=[
                        f"Endpoint: {endpoint}",
                        f"Method: {method}",
                        f"Payload: {failure.get('request', {}).get('body', 'N/A')}"
                    ],
                    evidence=[
                        str(failure.get("response", {}).get("status_code", "")),
                        str(failure.get("response", {}).get("body", ""))[:500]
                    ],
                    remediation="Review API endpoint validation and error handling",
                    source_tool="Schemathesis",
                    raw_finding=failure
                )
                findings.append(finding)

        except Exception as e:
            print(f"Error parsing Schemathesis results: {e}")
        return findings

    def parse_trivy_results(self, json_file: Path) -> List[Finding]:
        """Parse Trivy vulnerability scan results."""
        findings = []
        try:
            with open(json_file, "r") as f:
                data = json.load(f)

            for result in data.get("Results", []):
                for vuln in result.get("Vulnerabilities", []):
                    finding = Finding(
                        id=self.generate_id(),
                        title=f"{vuln.get('VulnerabilityID', 'Unknown')}: {vuln.get('Title', vuln.get('PkgName', 'Unknown'))}",
                        severity=vuln.get("Severity", "UNKNOWN").upper(),
                        category="Dependency Vulnerability",
                        description=vuln.get("Description", ""),
                        affected_component=f"{vuln.get('PkgName', 'Unknown')}@{vuln.get('InstalledVersion', 'Unknown')}",
                        reproduction_steps=[f"Package: {vuln.get('PkgName')}", f"Version: {vuln.get('InstalledVersion')}"],
                        evidence=[vuln.get("VulnerabilityID", "")],
                        remediation=f"Upgrade to {vuln.get('FixedVersion', 'latest version')}" if vuln.get('FixedVersion') else "Check for updates",
                        cwe_id=vuln.get("CweIDs", [None])[0] if vuln.get("CweIDs") else None,
                        cvss_score=vuln.get("CVSS", {}).get("nvd", {}).get("V3Score"),
                        references=vuln.get("References", [])[:5],
                        source_tool="Trivy",
                        raw_finding=vuln
                    )
                    findings.append(finding)
        except Exception as e:
            print(f"Error parsing Trivy results: {e}")
        return findings

    def parse_osv_results(self, json_file: Path) -> List[Finding]:
        """Parse OSV vulnerability correlation results."""
        findings = []
        try:
            with open(json_file, "r") as f:
                data = json.load(f)

            vulns = data if isinstance(data, list) else data.get("findings", data.get("vulnerabilities", []))

            for vuln in vulns:
                finding = Finding(
                    id=self.generate_id(),
                    title=vuln.get("title", vuln.get("id", "Unknown Vulnerability")),
                    severity=vuln.get("severity", "MEDIUM").upper(),
                    category="Dependency Vulnerability",
                    description=vuln.get("description", ""),
                    affected_component=f"{vuln.get('package', 'Unknown')}@{vuln.get('version', 'Unknown')}",
                    reproduction_steps=[f"Package: {vuln.get('package')}", f"Version: {vuln.get('version')}"],
                    evidence=vuln.get("cve_ids", []),
                    remediation=f"Upgrade to: {', '.join(vuln.get('fixed_versions', ['latest']))}" if vuln.get('fixed_versions') else "Check for updates",
                    cvss_score=vuln.get("cvss_score"),
                    references=vuln.get("references", [])[:5],
                    source_tool="OSV",
                    raw_finding=vuln
                )

                # Mark KEV findings
                if vuln.get("is_kev"):
                    finding.title = f"[KEV] {finding.title}"
                    finding.severity = "CRITICAL"

                findings.append(finding)
        except Exception as e:
            print(f"Error parsing OSV results: {e}")
        return findings

    def aggregate_all(self) -> List[Finding]:
        """Aggregate findings from all sources."""
        print(f"Scanning results directory: {self.scan_dir}")

        # Find and parse all result files
        parsers = {
            "**/nuclei*.json": self.parse_nuclei_results,
            "**/bandit*.json": self.parse_bandit_results,
            "**/semgrep*.json": self.parse_semgrep_results,
            "**/zap*.json": self.parse_zap_results,
            "**/jwt-test*.json": self.parse_custom_results,
            "**/auth-test*.json": self.parse_custom_results,
            "**/idor-test*.json": self.parse_custom_results,
            "**/xss-test*.json": self.parse_custom_results,
            "**/schemathesis*.json": self.parse_schemathesis_results,
            "**/trivy*.json": self.parse_trivy_results,
            "**/osv*.json": self.parse_osv_results,
            "**/grype*.json": self.parse_trivy_results,  # Similar format
        }

        for pattern, parser in parsers.items():
            for file in self.scan_dir.glob(pattern):
                print(f"Parsing: {file}")
                self.findings.extend(parser(file))

        # Deduplicate findings
        self.deduplicate()

        return self.findings

    def deduplicate(self):
        """Remove duplicate findings."""
        seen = set()
        unique = []
        for finding in self.findings:
            key = (finding.title, finding.affected_component, finding.severity)
            if key not in seen:
                seen.add(key)
                unique.append(finding)
        self.findings = unique

    def to_dict(self) -> Dict:
        """Convert findings to dictionary format."""
        return {
            "scan_date": datetime.now().isoformat(),
            "total_findings": len(self.findings),
            "by_severity": {
                "CRITICAL": len([f for f in self.findings if f.severity == "CRITICAL"]),
                "HIGH": len([f for f in self.findings if f.severity == "HIGH"]),
                "MEDIUM": len([f for f in self.findings if f.severity == "MEDIUM"]),
                "LOW": len([f for f in self.findings if f.severity == "LOW"]),
                "INFO": len([f for f in self.findings if f.severity == "INFO"]),
            },
            "findings": [asdict(f) for f in self.findings]
        }


def main():
    parser = argparse.ArgumentParser(
        description="Aggregate security scan findings into a unified report.",
        epilog="IMPORTANT: Always specify --scan-dir to use only one scan's results."
    )
    parser.add_argument(
        "--scan-dir",
        required=True,
        help="Path to the specific scan directory (e.g., /app/results/20260102_105341)"
    )
    parser.add_argument(
        "--output",
        help="Output file path (default: <scan-dir>/aggregated_findings.json)"
    )
    args = parser.parse_args()

    scan_dir = Path(args.scan_dir)
    if not scan_dir.exists():
        print(f"Error: Scan directory not found: {scan_dir}")
        return 1

    # Validate it's a specific scan directory, not the parent results folder
    if scan_dir.name == "results":
        print("Error: Please specify a specific scan directory, not /app/results")
        print("Example: --scan-dir /app/results/20260102_105341")
        return 1

    aggregator = FindingsAggregator(str(scan_dir))
    findings = aggregator.aggregate_all()

    output_file = Path(args.output) if args.output else scan_dir / "aggregated_findings.json"
    with open(output_file, "w") as f:
        json.dump(aggregator.to_dict(), f, indent=2)

    print(f"\nAggregated {len(findings)} findings")
    print(f"Output: {output_file}")
    return 0


if __name__ == "__main__":
    exit(main() or 0)
