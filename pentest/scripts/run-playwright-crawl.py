#!/usr/bin/env python3
"""
Playwright-based SPA crawler for discovering client-side routes.
Designed for Next.js App Router applications.

OpenCitiVibes Penetration Testing - Phase 1
"""

import argparse
import json
import os
import sys
from pathlib import Path
from urllib.parse import urljoin, urlparse

try:
    from playwright.sync_api import sync_playwright, TimeoutError as PlaywrightTimeout
    HAS_PLAYWRIGHT = True
except ImportError:
    HAS_PLAYWRIGHT = False
    print("Warning: Playwright not installed. Install with: pip install playwright", file=sys.stderr)


def parse_args():
    parser = argparse.ArgumentParser(description="Crawl SPA to discover routes")
    parser.add_argument("--start-url", required=True, help="Starting URL")
    parser.add_argument("--output-file", required=True, help="Output file for URLs")
    parser.add_argument("--login", help="Login credentials as email:password")
    parser.add_argument("--max-depth", type=int, default=3, help="Max crawl depth")
    parser.add_argument("--headless", type=str, default="true", help="Run headless (true/false)")
    parser.add_argument("--timeout", type=int, default=30000, help="Page timeout ms")
    parser.add_argument("--wait-after-load", type=int, default=2000, help="Wait ms after load")
    return parser.parse_args()


def login_to_app(page, base_url: str, credentials: str) -> dict:
    """Authenticate and return session cookies."""
    email, password = credentials.split(":", 1)

    # Navigate to login page (adjust path as needed)
    login_url = urljoin(base_url, "/login")
    page.goto(login_url, wait_until="networkidle")

    # Fill login form (adjust selectors for your app)
    # Try multiple selector strategies for compatibility
    try:
        # Try email input
        email_selectors = [
            'input[type="email"]',
            'input[name="email"]',
            'input[id="email"]',
            'input[placeholder*="email" i]'
        ]
        for selector in email_selectors:
            if page.locator(selector).count() > 0:
                page.fill(selector, email)
                break

        # Try password input
        password_selectors = [
            'input[type="password"]',
            'input[name="password"]',
            'input[id="password"]'
        ]
        for selector in password_selectors:
            if page.locator(selector).count() > 0:
                page.fill(selector, password)
                break

        # Click submit button
        submit_selectors = [
            'button[type="submit"]',
            'input[type="submit"]',
            'button:has-text("Login")',
            'button:has-text("Sign in")',
            'button:has-text("Connexion")'
        ]
        for selector in submit_selectors:
            if page.locator(selector).count() > 0:
                page.click(selector)
                break

        # Wait for navigation after login
        page.wait_for_load_state("networkidle")

    except Exception as e:
        print(f"Login form interaction failed: {e}", file=sys.stderr)

    # Get cookies for reuse
    cookies = page.context.cookies()
    return {"cookies": cookies, "logged_in": True}


def crawl_page(page, url: str, visited: set, urls: set, base_domain: str,
               depth: int, max_depth: int, wait_ms: int):
    """Recursively crawl page and collect internal links."""
    if depth > max_depth or url in visited:
        return

    visited.add(url)

    try:
        page.goto(url, wait_until="domcontentloaded", timeout=30000)
        page.wait_for_timeout(wait_ms)  # Wait for JS to render

        # Extract all links
        links = page.evaluate("""
            () => {
                const anchors = document.querySelectorAll('a[href]');
                return Array.from(anchors).map(a => a.href);
            }
        """)

        # Also extract Next.js router links (data-href attributes)
        next_links = page.evaluate("""
            () => {
                const nextLinks = document.querySelectorAll('[data-href]');
                return Array.from(nextLinks).map(el => el.getAttribute('data-href'));
            }
        """)
        links.extend([l for l in next_links if l])

        # Extract form actions
        form_actions = page.evaluate("""
            () => {
                const forms = document.querySelectorAll('form[action]');
                return Array.from(forms).map(f => f.action);
            }
        """)
        links.extend([l for l in form_actions if l])

        for link in links:
            if not link:
                continue
            parsed = urlparse(link)
            # Only follow internal links
            if parsed.netloc == base_domain or not parsed.netloc:
                # Normalize URL
                if not parsed.scheme:
                    link = urljoin(url, link)
                    parsed = urlparse(link)

                clean_url = f"{parsed.scheme}://{parsed.netloc}{parsed.path}"
                if clean_url not in urls:
                    urls.add(clean_url)
                    if clean_url not in visited:
                        crawl_page(page, clean_url, visited, urls, base_domain,
                                   depth + 1, max_depth, wait_ms)

    except PlaywrightTimeout:
        print(f"Timeout crawling: {url}", file=sys.stderr)
    except Exception as e:
        print(f"Error crawling {url}: {e}", file=sys.stderr)


def main():
    args = parse_args()

    if not HAS_PLAYWRIGHT:
        print("Error: Playwright is required but not installed.", file=sys.stderr)
        print("Install with: pip install playwright && playwright install chromium", file=sys.stderr)
        sys.exit(1)

    base_domain = urlparse(args.start_url).netloc
    visited = set()
    urls = set([args.start_url])
    session_data = {}

    headless = args.headless.lower() == "true"

    with sync_playwright() as p:
        browser = p.chromium.launch(headless=headless)
        context = browser.new_context(
            viewport={"width": 1920, "height": 1080},
            user_agent="Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        )
        page = context.new_page()
        page.set_default_timeout(args.timeout)

        # Login if credentials provided
        if args.login:
            print(f"Authenticating to {args.start_url}...", file=sys.stderr)
            try:
                session_data = login_to_app(page, args.start_url, args.login)
                print("Login successful", file=sys.stderr)
            except Exception as e:
                print(f"Login failed: {e}", file=sys.stderr)

        # Start crawling
        print(f"Starting crawl from {args.start_url}", file=sys.stderr)
        print(f"Max depth: {args.max_depth}", file=sys.stderr)
        crawl_page(page, args.start_url, visited, urls, base_domain,
                   0, args.max_depth, args.wait_after_load)

        browser.close()

    # Write URLs to output file
    output_path = Path(args.output_file)
    output_path.parent.mkdir(parents=True, exist_ok=True)

    with open(output_path, "w") as f:
        for url in sorted(urls):
            f.write(f"{url}\n")

    # Save session data if logged in
    if session_data:
        session_file = output_path.parent / "playwright-session.json"
        with open(session_file, "w") as f:
            json.dump(session_data, f, indent=2)

    print(f"Discovered {len(urls)} URLs, saved to {args.output_file}", file=sys.stderr)


if __name__ == "__main__":
    main()
